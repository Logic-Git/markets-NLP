{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from google.colab import files\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import re"
      ],
      "metadata": {
        "id": "VGD8rOiQeprM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Name Entity Recognition\n",
        "## Step 1:\n",
        "We start by loading the sample.txt file provided. Then we want to seperate the different news pieces or social media comments from each other. We do this by recognizing that most of the times, whenever a new comment or news piece is written in the txt file, it is started in a new line. Therefore, we consider that a new news piece or comment is being started whever we come across the new line character. We recognize that we need to do this if we need to count the number of articles that mention gold and silver. Also, this would be useful in solving other parts of the problem. An important note to remember for the rest of the markdown file is that whenever the word article is referred to in the rest of the file, it means a particular instance of a news piece or a comment taken from the txt file that has been given."
      ],
      "metadata": {
        "id": "NCehnZNiiMjv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naiX6yXwQAwa",
        "outputId": "ed354951-1db0-4542-e53e-bb5e98777876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1870\n"
          ]
        }
      ],
      "source": [
        "#We start by loading the sample.txt file provided. Then we want to seperate the different news pieces of social media comments from each other. We do this by recognizing that most of the times, whenever a new comment or news piece is written in the txt file, it is started in a new line. Therefore, we spereate the\n",
        "# Open the file in read mode\n",
        "with open('sample.txt', 'r') as file:\n",
        "    # Read the entire contents of the file\n",
        "    file_contents = file.read()\n",
        "\n",
        "# Seperate the different news pieces and social media articles from each other.\n",
        "articles = file_contents.split('\\n')\n",
        "print(len(articles))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(articles[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaoUgIqohCiU",
        "outputId": "3a239153-e769-43dd-8403-bc4d35f120bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@fansoniclove Gold the Tenrec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2:\n",
        "To find out whether an article contains gold or silver, we make a function. This function takes an aritcle. Then it seperates the article into words. The method that we use to seperate the article into words disregards things like capital letters and punctuation marks. Therefore, it prevents gold!, Gold and gold from being considered as seperate words for example. It convers all the words into lower case for ease while mathcing words. Then we just check if the words list obtained from the article contains 'gold' or 'silver'. We can be confident that 'gold' and 'silver' would probably contain most instances of the two words because of the preprocessing we did."
      ],
      "metadata": {
        "id": "uCNpN5VkmPzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function that takes in an article and tells if the article contains the word 'silver' or 'gold'.\n",
        "def string_matcher(article):\n",
        "  words = re.findall(r'\\b\\w+\\b', article.lower())                 #This helps seperate the article into unique words. It ignores punctuation and capital letters so Gold and gold are cosidered the same word and appear 1 time.\n",
        "  bool_sil = 0\n",
        "  bool_gol = 0\n",
        "  for i in range(len(words)):\n",
        "    if words[i].lower()=='silver':\n",
        "      bool_sil = 1\n",
        "    if words[i].lower()=='gold':\n",
        "      bool_gol = 1\n",
        "  return bool_sil,bool_gol\n"
      ],
      "metadata": {
        "id": "nf_BgUYga9Bc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Counts the total articles containing silver and the total articles containing gold and prints them.\n",
        "total_silver = 0\n",
        "total_gold = 0\n",
        "for i in range(len(articles)):\n",
        "  sil,gol = string_matcher(articles[i])\n",
        "  total_silver += sil\n",
        "  total_gold += gol\n",
        "print('Total number of articles containing the word silver are: ',total_silver)\n",
        "print('Total number of articles containing the word gold are: ', total_gold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd5BjtTk88Ld",
        "outputId": "ad5a277f-da76-464b-b046-e5ffdb8c507a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of articles containing the word silver are:  100\n",
            "Total number of articles containing the word gold are:  605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 Accounting for False Positives:\n",
        "Along with the example of 'silver lining' give in the question, an example of an instance of false positive is sentences where phrases like 'gold medal' are used. Along with wanting to disregard these instances, we also notice that there are several commonalities in false positives that we need to teach our program to disregard. An example is phrases of the form 'as gold/silver' or 'like gold/silver'. This is because in these contexts, gold and silver are probably not being used in their literal meaning but are used as a metaphor or a simile. An example sentence from everyday usage can be, 'Knowledge is as valuable as gold these days'. Here, clearly, gold is a false positive. This shows how our method of excluding these instances can be generalized to other sentences from everyday language. Since we are searching for the words 'gold' or 'silver', our previously used preprocessing technique would already ensure we do not count gold when it is in company names (for example Goldman Sachs because Goldman is not counted as gold). Finally, another thing that leads to false positives can be using these words while referring to paints and colours. We exclude these instances as well. These things also make our algorithm more generalizable because they deal with several different possible contexts of false positives that might pop up in everyday life.\n",
        "\n",
        "##Implementation:\n",
        "To implement this, we will make 3 lists. One list is for words that if come immediately before instance of silver or gold, we regard the article as false positive. The second is for words that if come immediately after gold or silver, then we regard the article as false positive. The final is for words which if they appear anywhere in the article, we regard it as false positive. By later adding or subtracting to these lists, we can make our algorithm more robust and generalizablle. An example of how the lists will be populated is that in the case of gold and silver being used as a part of a simili or a metaphor, they will probably have the word like or as immediately before them ('as valuable as gold'). Therefore, we add like and as to the list of words that come before. An important note is that this method can also be extended by adding more lists if that means better generalization. For example you can have different lists for gold and silver. This can help detect the silver lining false positive but prevent flagging a phrase like gold lining as false positive."
      ],
      "metadata": {
        "id": "DObUekiTAKyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function that takes in an article and gives 0 if it does not contain 'gold' or 'silver' and 1 otherwise. Excludes False Positives.\n",
        "def classify_excluding_false_positive(article,imediate_left_list,imediate_right_list,full_article_list):\n",
        "    words = re.findall(r'\\b\\w+\\b', article.lower())\n",
        "    for i in range(len(full_article_list)):\n",
        "      if full_article_list[i] in words:\n",
        "        return 0\n",
        "      else:\n",
        "        continue\n",
        "    if 'silver' in words or 'gold' in words:\n",
        "      for i in range(len(words)):            #We look through all the words so that if gold or silver appears in two instances, both instances are checked\n",
        "        if words[i]=='gold' or words[i]=='silver':\n",
        "          if i==0:\n",
        "            if words[i+1] in immediate_right_list:\n",
        "              pass\n",
        "            else:\n",
        "              return 1                       #All the return 1 statements are there so that as soon as we find gold or silver being used in the right context, we label the article as one. This prevents the article from being labelled as false positive for example if silver is used in that context again later in the sentence.\n",
        "          elif i==len(words)-1:\n",
        "            if words[i-1] in immediate_left_list:\n",
        "              pass\n",
        "            else:\n",
        "              return 1\n",
        "          elif words[i-1] in immediate_left_list or words[i+1] in immediate_right_list:\n",
        "            pass\n",
        "          else:\n",
        "            return 1\n",
        "        else:\n",
        "          continue\n",
        "    return 0"
      ],
      "metadata": {
        "id": "yt5jAZoW_-MV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "immediate_left_list = ['as','like']\n",
        "immediate_right_list = ['lining']\n",
        "full_article_list = ['paint', 'color','colour']\n",
        "sum = 0\n",
        "for i in range(len(articles)):\n",
        "  sum = sum+classify_excluding_false_positive(articles[i],immediate_left_list,immediate_right_list,full_article_list)\n",
        "#print(classify_excluding_false_positive('Books are as valuable as gold',immediate_left_list,immediate_right_list,full_article_list))\n",
        "print(\"Total articles containing silver or gold excluding false positives are: \", sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gId3_uj59FIk",
        "outputId": "1a68558e-0364-4e07-9fa2-15dec69d106f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total articles containing silver or gold excluding false positives are:  652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 4 Accounting for false negatives\n",
        "We account for false negatives by a simple extension of the function we made for false positives. The only difference is that we take in a list of aliases in our function for gold or silver. However, the important thing to not in implementation is that some of the aliases for gold and silvers are probably always used in trading environments (for example XAU/USD) so they will always refer to silver or gold as tradeable commodities. Therefore, there are some aliases that if present in the sentence should always be detected as silver or gold being used as tradeable commodities so they do not have the risk of being a false positive while other aliases that are just synonyms of gold and silver so they do have a risk of being a false positive. We handle this by passing to out function a list for trading environment aliases and a list for synonyms of gold and silver. Then we apply the false positive techniques only to the latter list. This helps us prevent missing the detection of sentences like 'I bought XAU/USD from my gold painted mobile phone.' Therefore, this makes our algorithm more generalizable."
      ],
      "metadata": {
        "id": "hxNX8F2wrP-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function that takes in an article and gives 0 if it does not contain 'gold' or 'silver' or any of its aliases and 1 otherwise. Excludes False Positives and includes False negatives.\n",
        "def classify_excluding_false_pos_neg(article,alias_trading,synonyms,imediate_left_list,imediate_right_list,full_article_list):\n",
        "    words = re.findall(r'\\b\\w+\\b', article.lower())\n",
        "    if any(name in words for name in alias_trading):  #If any of the trading names of gold and silver are observed, we immediately output 1 because they are probably being used as tradeable commodities.\n",
        "      return 1\n",
        "    if any(name in words for name in full_article_list):\n",
        "      return 0\n",
        "    if any(name in words for name in synonyms):\n",
        "      for i in range(len(words)):     #We look through all the words so that if gold or silver appears in two instances, both instances are checked\n",
        "        if words[i] in synonyms:\n",
        "          if i==0:\n",
        "            if words[i+1] in immediate_right_list:\n",
        "              pass\n",
        "            else:\n",
        "              return 1    #All the return 1 statements are there so that as soon as we find gold or silver being used in the right context, we label the article as one. This prevents the article from being labelled as false positive for example if silver is used in that context again later in the sentence.\n",
        "          elif i==len(words)-1:\n",
        "            if words[i-1] in immediate_left_list:\n",
        "              pass\n",
        "            else:\n",
        "              return 1\n",
        "          elif words[i-1] in immediate_left_list or words[i+1] in immediate_right_list:\n",
        "            pass\n",
        "          else:\n",
        "            return 1\n",
        "        else:\n",
        "          continue\n",
        "    return 0"
      ],
      "metadata": {
        "id": "z43IkqzNy24w"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alias_trading = ['xau','xag','xauusd','xagusd']\n",
        "synonyms = ['ingot','ingots','gold','silver']\n",
        "#print(classify_excluding_false_pos_neg('Lets buy XAU/USD from the market',alias_trading,synonyms,immediate_left_list,immediate_right_list,full_article_list))\n",
        "sum = 0\n",
        "for i in range(len(articles)):\n",
        "  sum = sum+classify_excluding_false_pos_neg(articles[i],alias_trading,synonyms,immediate_left_list,immediate_right_list,full_article_list)\n",
        "print(\"Total articles containing silver or gold or their aliases excluding false positives and false negatives are: \", sum)"
      ],
      "metadata": {
        "id": "hBAebyHp-I8T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1993a0b9-3b1f-415f-dc6f-9376a7150770"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total articles containing silver or gold or their aliases excluding false positives and false negatives are:  656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Better Method:\n",
        "Another method, which I believe can potentially be better than the above methods in detecting where silver and gold is being used as a tradeable commodity is to use a recurrent neural network. There are two main problems with our previous method. First, to see if the words gols and silver or their aliases are used as a tradeable commodity, there is a need to judge the overall context of the sentence. Our previous method only looks at the words to the immediate right or immediate left of the commodities. Moreover, for the cases where it looks at words throughout the article, it dismisses the article as a false positive without any contextual consideration. Therefore, our previous algorithm is very robotic. This can result in errors in detection. The second problem is that we cannot possible think of all the different contexts in which gold or silver or their alises are used as a tradeable commodity and we cannot think of all the false positive or false negative cases. However, the previous method hinges on us thinking of these instances and then puting the required words in the lists. Therefore, the previous method is not very generalizable. This is why we implement a recurrent neural network. Recurrent neural networks have the capability of taking in sequences of information and giving the final output while considering the sequence which is why it seems like a good idea.\n",
        "## Implementation\n",
        "In our implementation, we recognize that the first hurdle is that data is unlabelled. Therefore, we decide to label the data using Chat GPT. To do this, we first write the articles in a csv file and the articles column has a labels column next to it so Chat GPT can easily understand that we want to assign a label to each article."
      ],
      "metadata": {
        "id": "fBTbmpYxElgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the CSV file\n",
        "csv_file = \"articles_unlabelled.csv\"\n",
        "\n",
        "# Write the phrases to the CSV file\n",
        "with open(csv_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"Article\",\"Label\"])\n",
        "    for article in articles:\n",
        "        writer.writerow([article])\n",
        "\n",
        "print(\"Articles have been written to\", csv_file)\n",
        "\n",
        "files.download('articles_unlabelled.csv')       #Used to download the file from Google Colab\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "kQp_3cDKergn",
        "outputId": "f435b52e-1d2d-4cc6-f8b5-18fbfc274c5a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Articles have been written to articles_unlabelled.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e09d3958-2b54-47d3-8ec7-21f2e88ed4f9\", \"articles_unlabelled.csv\", 937871)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We explain to Chat GPT that we want to detect articles where silver and gold are mentioned as commodities. We tell it to label all such instances as 1 and other instances as 0. We also explain everything about false positive and false negatives and how it should label false positives and 0 and false negatives as 1 so that our RNN when trained on this data can recognize these instances and label them correctly. We expect ChatGPT to give better labelling results then our previous algorithm because it is an LLM and it is trained to detect context and sentiment in human language. However, after labelling, we will see that ChatGPT is still not as good as a human. In fact, it makes many mistakes. In general, during such a task one would always use human data labllers or human and AI hybrid data labelling to label the data. This was not done in this case because all this is just done as a proof of concept and there are resource and time limitations on the project. After getting the labelled data from ChatGPT, we read it, and make a vocabulary list of uniques words from it. From the vocabulary list, we will generate one-hot vectors for each word and allt he articles would be given to our RNN as a sequence of one-hot vectors representing the words in the articles. We ignore the punctuation marks. Our RNN itself takes in one-hot vectors and makes dense embeddings from them so that the vectors represent contxtual meaning as well. The RNN itself is designed such that it sequntially takes all the vectors and at the end of the article, provides log probabilities of the article belonging to one of the two classes: no silver/gold as commodities and silver/gold as commodities."
      ],
      "metadata": {
        "id": "bWVOOS5-pMsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "articles = []\n",
        "labels = []\n",
        "# Open the CSV file in read mode\n",
        "with open('articles_labelled.csv', 'r') as file:\n",
        "    # Create a CSV reader object\n",
        "    csv_reader = csv.reader(file)\n",
        "    ctr = 0\n",
        "    # Iterate over each row in the CSV file\n",
        "    for row in csv_reader:\n",
        "        if(ctr==0):\n",
        "          ctr = ctr+1\n",
        "          continue\n",
        "        # Each row is a list representing the columns in that row\n",
        "        # Access elements in the row list using indexing\n",
        "        if(row[0]==''):\n",
        "          continue\n",
        "        else:\n",
        "          articles.append(row[0])\n",
        "          labels.append(int(row[1]))\n"
      ],
      "metadata": {
        "id": "QcbyhHZkiqZb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocabulary_set = set()\n",
        "for article in articles:\n",
        "    # Basic preprocessing to remove punctuation and split by spaces\n",
        "    words = re.findall(r'\\b\\w+\\b', article.lower())\n",
        "    vocabulary_set.update(words)\n",
        "\n",
        "# Convert the set to a list and sort it to have a consistent order\n",
        "vocabulary_list = sorted(list(vocabulary_set))\n",
        "\n",
        "# Create a dictionary that maps words to indices\n",
        "vocabulary = {word: idx for idx, word in enumerate(vocabulary_list)}\n",
        "\n"
      ],
      "metadata": {
        "id": "4q0SqXOnppBP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(vocabulary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D-r4c8ErFXt",
        "outputId": "6f7b2e71-2b6f-45a4-a2ca-ed8c38657f4c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#device config\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    # implement RNN from scratch rather than using nn.RNN\n",
        "    def __init__(self, input_size, embed_dim,hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, embed_dim)     #used to create dense embedding from one-hot vecotrs\n",
        "        self.hidden_size = hidden_size\n",
        "        self.i2h = nn.Linear(embed_dim + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(embed_dim + hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input_tensor, hidden_tensor):\n",
        "        embeded_tensor = self.embedding(input_tensor)\n",
        "        if(len(embeded_tensor.size())==1):\n",
        "          embeded_tensor = torch.unsqueeze(embeded_tensor,0)\n",
        "        combined = torch.cat((embeded_tensor, hidden_tensor), 1)\n",
        "\n",
        "        hidden = self.i2h(combined)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.zeros(1, self.hidden_size)).to(device)     #initializes the hidden layer output for the first word\n",
        "all_categories = [0,1]\n",
        "n_categories = 2\n",
        "n_hidden = 128\n",
        "embed_dimension = 100    #one hot vectors of dimension 18046 (length of vocabulary list) converted to dense vectors of dimension 100\n",
        "rnn = RNN(len(vocabulary), embed_dimension, n_hidden, n_categories)\n",
        "rnn = rnn.to(device)\n",
        "\n",
        "\n",
        "'''def category_from_output(output):\n",
        "    category_idx = torch.argmax(output).item()\n",
        "    return all_categories[category_idx]'''\n",
        "\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "learning_rate = 0.005\n",
        "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
        "\n",
        "def train(article_tensor, category_tensor):    #The functions runs a training step for a single article and its category\n",
        "    hidden = rnn.init_hidden()\n",
        "    for i in range(len(article_tensor)):\n",
        "      output,hidden = rnn(article_tensor[i],hidden)\n",
        "    loss = criterion(output, category_tensor)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return output, loss.item()\n",
        "\n",
        "def select_article(articles,index,labels):    #The function is used to select article at a particular index and convert it into appropriate form and return it and its corresponding label\n",
        "  wrds = re.findall(r'\\b\\w+\\b', articles[index].lower())\n",
        "  wrds_idx = []\n",
        "  for j in range(len(wrds)):\n",
        "    wrds_idx.append(vocabulary.get(wrds[j]))\n",
        "  return (torch.tensor(wrds_idx)).to(device),(torch.tensor([labels[counter]])).to(device)\n",
        "\n",
        "def check_accuracy(article_arr,label_arr,model):   #Checks the accuracy of the trained rnn model on the entire dataset\n",
        "  sum_correct = 0\n",
        "  for i in range(len(article_arr)):\n",
        "    indices,category = select_article(article_arr,i,label_arr)\n",
        "    with torch.no_grad():\n",
        "      hidden = model.init_hidden()\n",
        "      for j in range(len(indices)):\n",
        "        output,hidden = model(indices[j],hidden)\n",
        "      pred_cat = torch.argmax(output)\n",
        "      if(pred_cat==label_arr[i]):\n",
        "        sum_correct+=1\n",
        "  return (sum_correct/len(article_arr))*100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "current_loss = 0\n",
        "all_losses = []\n",
        "print_steps = 25\n",
        "n_iters = 900\n",
        "counter = 0\n",
        "\n",
        "for i in range(n_iters):    #This helps loop through our list of articles\n",
        "    wrds_idx,category = select_article(articles,counter,labels)     #takes a particular article at a particular index, converts it into a suitable format so it can be fed into our rnn\n",
        "    counter = (counter+1)%len(labels)    #prevents counter from going out of range of the list of articles\n",
        "    if(len(wrds_idx)==0):\n",
        "      continue\n",
        "    output, loss = train(wrds_idx, category)\n",
        "    current_loss += loss\n",
        "\n",
        "    if (i+1) % print_steps == 0:        #prints some training statistics every few steps\n",
        "      print(\"The loss is: \",current_loss)\n",
        "      acc = check_accuracy(articles,labels,rnn)\n",
        "      print(\"The accuracy is: \",acc)\n",
        "      print(\"The step is: \", i+1)\n",
        "      current_loss = 0"
      ],
      "metadata": {
        "id": "JsjFoHtcYrM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95c68a8a-f9b1-4285-db90-28eb6eb899ba"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss is:  14.70780636370182\n",
            "The accuracy is:  56.470588235294116\n",
            "The step is:  25\n",
            "The loss is:  14.806015871465206\n",
            "The accuracy is:  55.42483660130719\n",
            "The step is:  50\n",
            "The loss is:  18.625802136957645\n",
            "The accuracy is:  55.751633986928105\n",
            "The step is:  75\n",
            "The loss is:  9.636103849858046\n",
            "The accuracy is:  55.032679738562095\n",
            "The step is:  100\n",
            "The loss is:  10.896702287718654\n",
            "The accuracy is:  55.947712418300654\n",
            "The step is:  125\n",
            "The loss is:  12.536292105913162\n",
            "The accuracy is:  54.90196078431373\n",
            "The step is:  150\n",
            "The loss is:  8.461463619023561\n",
            "The accuracy is:  54.18300653594771\n",
            "The step is:  175\n",
            "The loss is:  11.680743781849742\n",
            "The accuracy is:  52.28758169934641\n",
            "The step is:  200\n",
            "The loss is:  7.644193306565285\n",
            "The accuracy is:  52.54901960784314\n",
            "The step is:  225\n",
            "The loss is:  7.675816901959479\n",
            "The accuracy is:  51.83006535947713\n",
            "The step is:  250\n",
            "The loss is:  8.188517524395138\n",
            "The accuracy is:  51.37254901960784\n",
            "The step is:  275\n",
            "The loss is:  4.00129905808717\n",
            "The accuracy is:  50.91503267973856\n",
            "The step is:  300\n",
            "The loss is:  11.076196623966098\n",
            "The accuracy is:  50.71895424836601\n",
            "The step is:  325\n",
            "The loss is:  17.38638027012348\n",
            "The accuracy is:  52.09150326797386\n",
            "The step is:  350\n",
            "The loss is:  12.95173839572817\n",
            "The accuracy is:  53.26797385620915\n",
            "The step is:  375\n",
            "The loss is:  3.545275171287358\n",
            "The accuracy is:  53.20261437908497\n",
            "The step is:  400\n",
            "The loss is:  12.271900129970163\n",
            "The accuracy is:  55.88235294117647\n",
            "The step is:  425\n",
            "The loss is:  45.998013600707054\n",
            "The accuracy is:  62.94117647058823\n",
            "The step is:  475\n",
            "The loss is:  12.04827232658863\n",
            "The accuracy is:  64.77124183006536\n",
            "The step is:  500\n",
            "The loss is:  17.347085889428854\n",
            "The accuracy is:  66.66666666666666\n",
            "The step is:  525\n",
            "The loss is:  10.543984539806843\n",
            "The accuracy is:  67.18954248366013\n",
            "The step is:  550\n",
            "The loss is:  11.466748744249344\n",
            "The accuracy is:  67.7124183006536\n",
            "The step is:  575\n",
            "The loss is:  17.664350062143058\n",
            "The accuracy is:  67.97385620915033\n",
            "The step is:  625\n",
            "The loss is:  15.240817539393902\n",
            "The accuracy is:  67.58169934640523\n",
            "The step is:  650\n",
            "The loss is:  11.503585666418076\n",
            "The accuracy is:  67.90849673202615\n",
            "The step is:  675\n",
            "The loss is:  10.738032860681415\n",
            "The accuracy is:  65.81699346405229\n",
            "The step is:  700\n",
            "The loss is:  12.830350168602308\n",
            "The accuracy is:  63.0718954248366\n",
            "The step is:  725\n",
            "The loss is:  13.486381363123655\n",
            "The accuracy is:  66.1437908496732\n",
            "The step is:  750\n",
            "The loss is:  24.926041815429926\n",
            "The accuracy is:  69.54248366013071\n",
            "The step is:  775\n",
            "The loss is:  13.37570344377309\n",
            "The accuracy is:  70.06535947712418\n",
            "The step is:  800\n",
            "The loss is:  14.855418998748064\n",
            "The accuracy is:  70.3921568627451\n",
            "The step is:  825\n",
            "The loss is:  6.573043663054705\n",
            "The accuracy is:  70.13071895424837\n",
            "The step is:  850\n",
            "The loss is:  14.445420755073428\n",
            "The accuracy is:  71.50326797385621\n",
            "The step is:  875\n",
            "The loss is:  13.826899242354557\n",
            "The accuracy is:  71.63398692810458\n",
            "The step is:  900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(article,vocabulary,model):       #After training, this function can be give any article. It can then output its prediction regarding that article.\n",
        "  words = re.findall(r'\\b\\w+\\b', article.lower())\n",
        "  indices = []\n",
        "  for i in range(len(words)):\n",
        "    if (vocabulary.get(words[i])==None):\n",
        "      continue\n",
        "    else:\n",
        "      indices.append(vocabulary.get(words[i]))\n",
        "  indices = (torch.tensor(indices)).to(device)\n",
        "  hidden = model.init_hidden()\n",
        "  for i in range(len(indices)):\n",
        "    output,hidden = model(indices[i],hidden)\n",
        "  pred_cat = torch.argmax(output)\n",
        "  if(pred_cat==0):\n",
        "    print('The string does not contain the words gold or silver being referred to as a tradeable commodity')\n",
        "  else:\n",
        "    print('The string contains either gold or silver being referred to as a tradeable commodity')\n"
      ],
      "metadata": {
        "id": "OGYRzaGWESIp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here it is important to not that there are several ways in which the training process of the model can be improved. They are listed below:\n",
        "For example, the first thing I would do is to chage the code a bit such that\n",
        "\n",
        "1.   As mentioned before, use human data labellers for better model performance on every day speech.\n",
        "2.   Change the code a bit so model can be trained on batched data to reap the benefits of parallel processing capabilities of pytorch. This would help reduce the training time. For now, a trained state of the model has been provided and it is loaded in the next cell so the person evaluating this notebook can just run the next cell without going through all the above training steps and evaluate the model and its performance easily and play aroung with it.\n",
        "3.   Split the data into training, validation and testing data. Tune hyperparameters using validation data and evaluate accuracy using testing data to check how well the model performs in general.\n",
        "4.   We see that in our training loop, the number of iterations are so low they do not even go through the entire data and the accuracy is only about 70%. This is because the gradients exploded in later iterations. This can be avoided easily by experimenting around with other optimizers (for example SGD can be used with weight decay and momentum or Adam optimizer can be used). If this is done then the neural network can achieve much higher accuracies.\n",
        "\n",
        "The reason why all this has not been done is due to time and resource constraints. At the end of the day, this is not intended to be a final product but it is just there to show the evaluator what my conception of a much better solution than the one above would be."
      ],
      "metadata": {
        "id": "HjPxx_RYeez_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(rnn.state_dict(),'rnn.pth')    #Used to save the trained model"
      ],
      "metadata": {
        "id": "1lIsIf4ji-07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This code block and the immediate next one can be run without running the previous training code block because they use the saved model provided with the jupyter notebook. Intended to save time of the evaluator.\n",
        "#device config\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "class RNN(nn.Module):\n",
        "    # implement RNN from scratch rather than using nn.RNN\n",
        "    def __init__(self, input_size, embed_dim,hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, embed_dim)     #used to create dense embedding from one-hot vecotrs\n",
        "        self.hidden_size = hidden_size\n",
        "        self.i2h = nn.Linear(embed_dim + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(embed_dim + hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input_tensor, hidden_tensor):\n",
        "        embeded_tensor = self.embedding(input_tensor)\n",
        "        if(len(embeded_tensor.size())==1):\n",
        "          embeded_tensor = torch.unsqueeze(embeded_tensor,0)\n",
        "        combined = torch.cat((embeded_tensor, hidden_tensor), 1)\n",
        "\n",
        "        hidden = self.i2h(combined)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.zeros(1, self.hidden_size)).to(device)     #initializes the hidden layer output for the first word\n",
        "n_categories = 2\n",
        "n_hidden = 128\n",
        "embed_dimension = 100\n",
        "rnn_loaded = RNN(len(vocabulary), embed_dimension, n_hidden, n_categories)\n",
        "rnn_loaded.load_state_dict(torch.load('rnn.pth',map_location=torch.device(device)))\n",
        "print('The accuracy of this model is: ', check_accuracy(articles,labels,rnn_loaded))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcnKhrWsjMTI",
        "outputId": "46919b07-7591-4566-de82-4c711d47f456"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of this model is:  70.84967320261438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Can be used to play around with the saved model\n",
        "predict('I just bought gold. I am happy.', vocabulary, rnn_loaded)\n",
        "predict('The silver lining to all the tough times was that I became a better person',vocabulary,rnn_loaded)\n",
        "predict('I am going up the mountain.',vocabulary,rnn_loaded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kD5LchmhSAnX",
        "outputId": "aee00f3d-3356-40de-a2f6-7e5e001920dd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The string contains either gold or silver being referred to as a tradeable commodity\n",
            "The string does not contain the words gold or silver being referred to as a tradeable commodity\n",
            "The string does not contain the words gold or silver being referred to as a tradeable commodity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 5 Sentiment Analysis:\n",
        "To do this, the strategy we use is that we make a dictionary of negative sentiment and positive sentiment words. For every article, we keep a sentiment score and we increment the score if positive sentiment words are found and decrement the score if negative sentiment words are found. At the end, we make 4 classes according to sentiment score. For example if the sentiment score is very high, we conclude that the article has very positive sentiment. This not only tells us the sentiment represented in the articles but also how positive or negative the sentiment is. If sentiment is very positive, we can be assured that the article reflects positive expectations from gold or silver. The limitations of this method again is first that it cannot possible assess the context accurately in general situations. An example is 'We expect Apple stock to go up but gold to go down.' Our method would flag this as neutral but actually it indicates negative sentiment towards gold. One way to prevent this can be to assign higher sentiment score when words are closer to 'gold' or 'silver' but this too is not very generalizable. Moreover, we also cannot possibly type up a python dictionary containing all the negative and positive sentiment words. To counter these problems, we can again use a RNN very similar to the one in the previous problem. The only difference would be the labelling of data and that the output of RNN would be of size 5 representing the 5 classes: very positive, positve, neutral, very negative, negative. Due to the advantages of RNNs described above, given enough data, the RNN would capture the complicated contexts and sentiments of human language much better than any hard coded dictionary."
      ],
      "metadata": {
        "id": "HjhqinHX0rpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "articles_gold = []    #Seperate articles that mention gold or silver in the context of being a tradeable commodity according to our first algorithms\n",
        "for i in range(len(articles)):\n",
        "  if classify_excluding_false_pos_neg(articles[i],alias_trading,synonyms,immediate_left_list,immediate_right_list,full_article_list)==1:\n",
        "    articles_gold.append(articles[i])\n",
        "sentiment_dict = {    #We plan to maintain a sentiment score. Negetive words decrease the score and positive words increase the score.\n",
        "    'rise': 1,\n",
        "    'gain': 1,\n",
        "    'upward': 1,\n",
        "    'bullish': 1,\n",
        "    'profit': 1,\n",
        "    'surge': 1,\n",
        "    'increase': 1,\n",
        "    'growth': 1,\n",
        "    'positive': 1,\n",
        "    'improve': 1,\n",
        "    'rally': 1,\n",
        "    'fall': -1,\n",
        "    'drop': -1,\n",
        "    'downward': -1,\n",
        "    'bearish': -1,\n",
        "    'loss': -1,\n",
        "    'decline': -1,\n",
        "    'decrease': -1,\n",
        "    'slump': -1,\n",
        "    'negative': -1,\n",
        "    'dip': -1,\n",
        "    'crash': -1\n",
        "}\n",
        "neutral = 0\n",
        "positive = 0\n",
        "negative = 0\n",
        "very_positive = 0\n",
        "very_negative = 0\n",
        "for i in range(len(articles_gold)):\n",
        "  words = re.findall(r'\\b\\w+\\b', articles_gold[i].lower())\n",
        "  sentiment_score = 0\n",
        "  for word in words:\n",
        "    if word in sentiment_dict.keys():\n",
        "      sentiment_score = sentiment_dict.get(word) + sentiment_score\n",
        "  if(sentiment_score>1):     #Sepereation of articles according to 4 different sentiment scores.\n",
        "    very_positive+=1\n",
        "  elif(sentiment_score<-1):\n",
        "    very_negative+=1\n",
        "  elif(sentiment_score==-1):\n",
        "    negative+=1\n",
        "  elif(sentiment_score==0):\n",
        "    neutral+=1\n",
        "  elif(sentiment_score==1):\n",
        "    positive+=1\n",
        "print('The number of aritcles where the sentiment is very positive are: ', very_positive)\n",
        "print('The number of aritcles where the sentiment is positive are: ', positive)\n",
        "print('The number of aritcles where the sentiment is neutral are: ', neutral)\n",
        "print('The number of aritcles where the sentiment is negative are: ', negative)\n",
        "print('The number of aritcles where the sentiment is very negative are: ', very_negative)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Xzjopyr1FxS",
        "outputId": "4cf1197e-99c5-4b85-b23e-a89e26c6e272"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of aritcles where the sentiment is very positive are:  27\n",
            "The number of aritcles where the sentiment is positive are:  13\n",
            "The number of aritcles where the sentiment is neutral are:  582\n",
            "The number of aritcles where the sentiment is negative are:  21\n",
            "The number of aritcles where the sentiment is very negative are:  13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Aliases Task\n",
        "To find reference to Broadcom Inc since 2008 from any given textual data, the first step is to conduct research on the different name changes that the company went through. To do this, we use Bing chat and explain to it that some places where name changes might be found are mergers, demergers, acquisitions or restructurings to get better results. Moreover, we search the web as well for more information. The reason we use Bing Chat instead of Chat GPT for this is that Bing Chat has web search capabilities so it can provide latest results while Chat GPT has a limitation on how much recent information it has due to its dataset and the fact that it does not have capabilities to search the internet.\n",
        "##Next Steps:\n",
        "Infromation gained from our research tells us that the roots of Broadcom Inc can be traced back to a company called Avago Technologies. This company existed from 2005 to 2016 so it is witin the time window that we are interested. In 2016, Avago Technologies merged with Broadcom Corporation to form Broadcom Limited. This would later change its name to form the present Broadcom Inc. Therefore, Broadcom Inc. has its roots in both Avago Technologies and Broadcom Corporation since in 2008-2016, it existed as two seperate companies that were later merged. Since we want to track mentions of Broadcom Inc it also makes sense to track mentions of Avago Technologies and Broadcom Corporation because Broadcom Inc now encapsulates both these companies. Therefore, we can simply make a python list containing the names 'Avago Technologies' and 'Broadcom Corporation' and 'Broadcom Inc', which are all the names that the company has taken since 2008. We also include 'BRCM' in the list since Broadcom Corporation was a publicly traded company with this ticker symbol. Finally, we include 'AVGO' since that is the current ticker symbol for Broadcom Inc. Avago Technologies does have previos name change history but it took place in 2005 so it is outside the scope of our considerantion. Broadcom Corporation does not have any other significant name change history before the merger otherwise we might have had to add more names to the list. After finalizing the list using the research we conducted, we just look for names in the list trough our data to look for mentions of Broadcom Inc since 2008 just like we did with 'gold' and 'silver'.  "
      ],
      "metadata": {
        "id": "kXxv7B51GnS3"
      }
    }
  ]
}